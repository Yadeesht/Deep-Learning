{"cells":[{"cell_type":"markdown","metadata":{"id":"wilfwh0hof8g"},"source":["Visit tensorflow HUB to know the existing models"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"mCcOxC_Cetg0","executionInfo":{"status":"ok","timestamp":1726593953920,"user_tz":-330,"elapsed":13076,"user":{"displayName":"Yadeesh T","userId":"06213349329621195536"}}},"outputs":[],"source":["import numpy as np\n","import os\n","\n","import PIL.Image as Image\n","import matplotlib.pylab as plt\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","import tensorflow_hub as hub\n","import cv2\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"MNLPmJfscJnf","executionInfo":{"status":"ok","timestamp":1726593953920,"user_tz":-330,"elapsed":7,"user":{"displayName":"Yadeesh T","userId":"06213349329621195536"}}},"outputs":[],"source":["d_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n","d_dir = tf.keras.utils.get_file(\"flower_photos\", origin=d_url, cache_dir=\"/content/drive/MyDrive\", untar=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1726593953920,"user":{"displayName":"Yadeesh T","userId":"06213349329621195536"},"user_tz":-330},"id":"uCEMGQcsfB_J","outputId":"9a11fa2f-d4a6-4fa1-95f7-0104b3e5e085"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["PosixPath('/content/drive/MyDrive/datasets/flower_photos')"]},"metadata":{},"execution_count":3}],"source":["import pathlib\n","d_dir = pathlib.Path(d_dir)\n","d_dir"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6061,"status":"ok","timestamp":1726593959976,"user":{"displayName":"Yadeesh T","userId":"06213349329621195536"},"user_tz":-330},"id":"__gd5lCigbp2","outputId":"e1585b89-a4bf-4115-c82a-4e0a3a531b0e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[PosixPath('/content/drive/MyDrive/datasets/flower_photos/roses/7345657862_689366e79a.jpg'),\n"," PosixPath('/content/drive/MyDrive/datasets/flower_photos/roses/7409458444_0bfc9a0682_n.jpg'),\n"," PosixPath('/content/drive/MyDrive/datasets/flower_photos/roses/9337528427_3d09b7012b.jpg'),\n"," PosixPath('/content/drive/MyDrive/datasets/flower_photos/roses/7551637034_55ae047756_n.jpg'),\n"," PosixPath('/content/drive/MyDrive/datasets/flower_photos/roses/5736328472_8f25e6f6e7.jpg')]"]},"metadata":{},"execution_count":4}],"source":["list(d_dir.glob(\"**/*.jpg\"))[:5]"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"rALtdKnPfvRO","executionInfo":{"status":"ok","timestamp":1726593959976,"user_tz":-330,"elapsed":16,"user":{"displayName":"Yadeesh T","userId":"06213349329621195536"}}},"outputs":[],"source":["data_image_dict={\n","    \"roses\": list(d_dir.glob(\"roses/*\")),\n","    \"daisy\": list(d_dir.glob(\"daisy/*\")),\n","    \"dandelion\": list(d_dir.glob(\"dandelion/*\")),\n","    \"sunflowers\": list(d_dir.glob(\"sunflowers/*\")),\n","    \"tulips\": list(d_dir.glob(\"tulips/*\")),\n","}"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"7QQLBaiNgkfT","executionInfo":{"status":"ok","timestamp":1726593959976,"user_tz":-330,"elapsed":15,"user":{"displayName":"Yadeesh T","userId":"06213349329621195536"}}},"outputs":[],"source":["label={\n","    \"roses\": 0,\n","    \"daisy\": 1,\n","    \"dandelion\": 2,\n","    \"sunflowers\": 3,\n","    \"tulips\": 4,\n","}\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1726593959976,"user":{"displayName":"Yadeesh T","userId":"06213349329621195536"},"user_tz":-330},"id":"san0lkAlhG6M","outputId":"8e525d49-75e2-4ad2-ae4f-76664a0c58fc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["3670"]},"metadata":{},"execution_count":7}],"source":["img_count=sum(map(len, data_image_dict.values()))\n","img_count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdpFbuOfnvXE"},"outputs":[],"source":["IMAGE_SIZE=(224,224)\n","x,y=[],[]\n","for fn,im in data_image_dict.items():\n","  for i in im:\n","    img=cv2.imread(str(i))\n","    r_img=cv2.resize(img,IMAGE_SIZE)\n","    x.append(r_img)\n","    y.append(label[fn])"]},{"cell_type":"code","source":["x=np.array(x)\n","y=np.array(y)"],"metadata":{"id":"9SQNEbZaxd7G"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uMLTov88mlIg"},"outputs":[],"source":["from sklearn.model_selection import train_test_split as tts\n","x_train,x_test,y_train,y_test=tts(x,y,random_state=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eOr_1CPjm8Uy"},"outputs":[],"source":["x_train=x_train/255\n","x_test=x_test/255"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8OPa7P2vq0UE"},"outputs":[],"source":["plt.axis(\"off\")\n","plt.imshow(x[0])"]},{"cell_type":"code","source":["import tensorflow as tf\n","from PIL import Image\n","import numpy as np\n","\n","# Load the MobileNetV2 model\n","model = tf.keras.applications.MobileNetV2(\n","    input_shape=(180, 180, 3),  # Define the input shape\n","    alpha=1.0,\n","    include_top=False,          # Set 'include_top=False' if you don't want the classification layers\n","    weights='imagenet',\n","    pooling='avg',              # You can use 'avg' or 'max' pooling for feature extraction\n","    classes=5,\n","    classifier_activation='softmax'\n",")\n","\n","# Load and preprocess the image\n","size = (180, 180)\n","img = Image.open(\"/content/goldfish.jpg\").resize(size)\n","\n","# Convert the image to a NumPy array and normalize it\n","img = np.array(img) / 255.0\n","\n","# Expand dimensions to match the expected input shape of (batch_size, height, width, channels)\n","img = np.expand_dims(img, axis=0)\n","\n","# Make predictions\n","predictions = model.predict(img)\n","\n","# Print the prediction results\n","pre=np.argmax(predictions)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C0u6YMoH6tM-","executionInfo":{"status":"ok","timestamp":1726593246196,"user_tz":-330,"elapsed":3057,"user":{"displayName":"Yadeesh T","userId":"06213349329621195536"}},"outputId":"ebc666c4-aa28-455e-b9e7-8fd05af7dd5b"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-53-7f1b59570e67>:6: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n","  model = tf.keras.applications.MobileNetV2(\n","WARNING:tensorflow:5 out of the last 32 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fb8282997e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"]}]},{"cell_type":"code","source":["pre"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yr6l6aOh-YYa","executionInfo":{"status":"ok","timestamp":1726593422689,"user_tz":-330,"elapsed":556,"user":{"displayName":"Yadeesh T","userId":"06213349329621195536"}},"outputId":"16cb34ea-c525-4a7c-f2c9-ece50551819b"},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["949"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Load the MobileNetV2 model without the top layer (include_top=False)\n","base_model = tf.keras.applications.MobileNetV2(\n","    input_shape=(224, 224, 3),  # Image size (224x224x3)\n","    include_top=False,          # Do not include the final dense layer\n","    weights='imagenet'          # Load weights pre-trained on ImageNet\n",")\n","\n","# Freeze the base model so that its weights are not updated during training\n","base_model.trainable = False\n","\n","# Add custom layers for the flower classification task\n","model = models.Sequential([\n","    base_model,\n","    layers.GlobalAveragePooling2D(),  # Add a pooling layer\n","    layers.Dense(128, activation='relu'),  # Add a fully connected layer\n","    layers.Dropout(0.5),  # Add dropout for regularization\n","    layers.Dense(5, activation='softmax')  # Output layer with 5 classes (for your flower dataset)\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',  # Use categorical cross-entropy for multi-class classification\n","              metrics=['accuracy'])\n","\n"],"metadata":{"id":"EWEm4nCR-lRi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","history = model.fit(\n","    x_train,y_train,\n","    epochs=10,  # Set the number of epochs\n","\n",")\n"],"metadata":{"id":"XQW7ohBe_1Ww"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oAUZJ3Nt_8vU","executionInfo":{"status":"ok","timestamp":1726593818522,"user_tz":-330,"elapsed":461,"user":{"displayName":"Yadeesh T","userId":"06213349329621195536"}},"outputId":"e3952716-1ee0-470c-e28e-72d689b24b6e"},"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([2, 0, 1, 2, 4])"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":[],"metadata":{"id":"58X-NmGTApG_"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1AgHs9IiJKMBqjqo07_lRi-jJbv7EaFMU","authorship_tag":"ABX9TyNKS0qsYfI80g9btD2KNL1C"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}